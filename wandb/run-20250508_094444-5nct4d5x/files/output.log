MultiModalCaptioner(
  (backbone): CLIPBackbone(
    (clip): CLIPModel(
      (text_model): CLIPTextTransformer(
        (embeddings): CLIPTextEmbeddings(
          (token_embedding): Embedding(49408, 512)
          (position_embedding): Embedding(77, 512)
        )
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-11): 12 x CLIPEncoderLayer(
              (self_attn): CLIPSdpaAttention(
                (k_proj): Linear(in_features=512, out_features=512, bias=True)
                (v_proj): Linear(in_features=512, out_features=512, bias=True)
                (q_proj): Linear(in_features=512, out_features=512, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=512, out_features=2048, bias=True)
                (fc2): Linear(in_features=2048, out_features=512, bias=True)
              )
              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
          (position_embedding): Embedding(50, 768)
        )
        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-11): 12 x CLIPEncoderLayer(
              (self_attn): CLIPSdpaAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
              )
              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (visual_projection): Linear(in_features=768, out_features=512, bias=False)
      (text_projection): Linear(in_features=512, out_features=512, bias=False)
    )
  )
  (decoder): MMDecoder(
    (type_emb): Embedding(2, 256)
    (pos_emb): Embedding(127, 256)
    (img_proj): Linear(in_features=768, out_features=256, bias=False)
    (txt_proj): Linear(in_features=512, out_features=256, bias=False)
    (blocks): ModuleList(
      (0-2): 3 x CausalSelfAttnBlock(
        (q_proj): Linear(in_features=256, out_features=256, bias=False)
        (k_proj): Linear(in_features=256, out_features=256, bias=False)
        (v_proj): Linear(in_features=256, out_features=256, bias=False)
        (out_proj): Linear(in_features=256, out_features=256, bias=False)
        (attn_drop): Dropout(p=0.1, inplace=False)
        (resid_drop): Dropout(p=0.1, inplace=False)
        (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=256, out_features=512, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=512, out_features=256, bias=True)
          (3): Dropout(p=0.1, inplace=False)
        )
        (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=256, out_features=49408, bias=True)
)
Total parameters: 165,914,625
Trainable parameters: 14,637,312
Epoch 1:  72%|███████████████████████████████████████████████████████████████████████▋                            | 6500/9063 [10:19<04:04, 10.49it/s, loss=1.123]
Traceback (most recent call last):
  File "/root/multi-modal-transfer-learning/train_decoder.py", line 145, in <module>
    main()
  File "/root/multi-modal-transfer-learning/train_decoder.py", line 91, in main
    logits, _ = model(pixel, inp)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/multi-modal-transfer-learning/model.py", line 206, in forward
    h_dec = self.decoder(h_img, h_txt, caption_input_ids) #(B, L_total, d_model)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/multi-modal-transfer-learning/model.py", line 174, in forward
    causal_mask[L_v:, L_v:] = torch.tril(torch.zeros(L_t, L_t))
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/multi-modal-transfer-learning/train_decoder.py", line 145, in <module>
    main()
  File "/root/multi-modal-transfer-learning/train_decoder.py", line 91, in main
    logits, _ = model(pixel, inp)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/multi-modal-transfer-learning/model.py", line 206, in forward
    h_dec = self.decoder(h_img, h_txt, caption_input_ids) #(B, L_total, d_model)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/multi-modal-transfer-learning/model.py", line 174, in forward
    causal_mask[L_v:, L_v:] = torch.tril(torch.zeros(L_t, L_t))
KeyboardInterrupt
